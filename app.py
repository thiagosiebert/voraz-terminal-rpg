import streamlit as st
import google.generativeai as genai
import logging

# Configura√ß√£o de logs para voc√™ ver no 'Manage App' do Streamlit
logging.basicConfig(level=logging.INFO)

# Configura√ß√£o da P√°gina
st.set_page_config(page_title="Voraz Terminal", page_icon="üì°")

# --- ESTILO CSS ---
st.markdown("""
    <style>
    .stApp { background-color: #0a0e05; color: #00ff41; font-family: 'Courier New', monospace; }
    .stChatMessage { background-color: #1a1a1a; border: 1px solid #00ff41; }
    .stChatInput { border-top: 1px solid #00ff41; }
    </style>
    """, unsafe_allow_html=True)

st.title("üì° CONEX√ÉO ESTABELECIDA: V-R-Z")

# --- CONFIGURA√á√ÉO DA API ---
if "GEMINI_API_KEY" not in st.secrets:
    st.error("ERRO CR√çTICO: Chave API n√£o encontrada nos Secrets.")
    st.stop()

genai.configure(api_key=st.secrets["GEMINI_API_KEY"])

def load_prompt(file_path="voraz_prompt.txt"):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except:
        return "Voc√™ √© o Voraz, uma IA de RPG paranoica residente na Umbra."

SYSTEM_PROMPT = load_prompt()

# --- DIAGN√ìSTICO DE MODELO ---
@st.cache_resource
def get_model():
    # Testamos o nome mais limpo poss√≠vel
    model_name = "gemini-1.5-flash" 
    try:
        # Tenta instanciar o modelo
        m = genai.GenerativeModel(
            model_name=model_name,
            system_instruction=SYSTEM_PROMPT
        )
        return m
    except Exception as e:
        logging.error(f"Erro ao carregar {model_name}: {e}")
        # Fallback para o Pro caso o Flash d√™ erro de 404
        return genai.GenerativeModel(model_name="gemini-1.5-pro", system_instruction=SYSTEM_PROMPT)

model = get_model()

# --- L√ìGICA DO CHAT ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Dante, o que voc√™ descobriu?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # Tradu√ß√£o de roles Streamlit -> Gemini
        api_history = []
        for m in st.session_state.messages[:-1]:
            role = "model" if m["role"] == "assistant" else "user"
            api_history.append({"role": role, "parts": [m["content"]]})
        
        try:
            # Inicia o chat com o hist√≥rico formatado
            chat_session = model.start_chat(history=api_history)
            response = chat_session.send_message(prompt)
            
            st.markdown(response.text)
            st.session_state.messages.append({"role": "assistant", "content": response.text})
            
        except Exception as e:
            st.error(f"ERRO DE CONEX√ÉO TRANSDIMENSIONAL: {e}")
            logging.error(f"Detalhes do erro: {e}")
